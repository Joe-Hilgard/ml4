---
title             : "ML4 Results Section in rMarkdown"
shorttitle        : "ML4 Results"

author: 
  - name          : "Richard A. Klein"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : ""
    email         : "raklein22@gmail.com"

affiliation:
  - id            : "1"
    institution   : "Universit√© Grenoble Alpes"

authornote: |
  This script generates the participants + results sections for the main ML4 manuscript. To knit this document you must install the papaja package from GitHub.

abstract: |

keywords          : "Terror Management Theory, mortality salience, replication, many labs"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : yes
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_word
---

```{r setup, include = FALSE}
# load packages
library("papaja")
library("metafor")
library("metaSEM")
library("haven")
library("psych")
library("dplyr")
library("effsize")
library("GPArotation")
library("tidyverse")
library("pwr")

# source functions locally
source("sources/numbers2words.r")

firstup <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}

# for calling formatC with desired arguments
formatCC <- function(x) formatC(x, big.mark = ",") 

report_meta3 <- function(object, term, label = "*Hedges' g*") {
  if (!(summary(object)$Mx.status1 %in% c(0, 1))) {
    return("the model failed to converge.")
    break
  }
  
  report_meta(object, term, label)
}

# for reporting results from object of class "meta3" in tidy fashion in-line
report_meta <- function(object, term, label = "*Hedges' g*") {
  # grab coefficients table
  res.df <- summary(object)$coefficients
  # fetch model stats
  g <- res.df[term, "Estimate"]
  g.lbound <- res.df[term, "lbound"]
  g.ubound <- res.df[term, "ubound"]
  se <- res.df[term, 'Std.Error']
  z <- res.df[term, 'z value']
  p <- res.df[term, 'Pr(>|z|)']
  
  # paste together into report
  paste0(label, " = ", printnum(g),
         ", 95% CI = [", printnum(g.lbound), ", ", printnum(g.ubound), 
         "], *SE* = ", printnum(se), 
         ", *Z* = ", printnum(z),
         ", *p* = ", printp(p))
}

report_q <- function(object) {
  # grab Q.stat list
  res.df <- summary(object)$Q.stat
  # fetch the numbers
  paste0("*Q*(", res.df$Q.df,
         ") = ", printnum(res.df$Q),
         ", *p* = ", printp(res.df$pval))
}
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(1)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r analysis-loaddata, include = FALSE}
# Reading in all necessary data
# Note: Some analyses require confidential (for participant identification concerns) data files. You'll have to comment out those lines for this file to knit. Contact Rick raklein22@gmail.com for information about getting the raw data, which usually simply requires the researcher to obtain IRB/ethics approval from their university stating they will maintain the confidentiality of any sensitive data.

# Primary data file with replication data aggregated across labs (deidentified is missing just a couple variables for this -- age and gender)
# Note: This file is the merged data provided by sites, produced by 001_data_cleaning.R
merged <- readRDS("./data/processed_data/merged_full.rds")
# Alternatively, you can run it with the public data and get most results. The problem with RMarkdown is that the script
# needs to run completely with no errors or it won't render. So, if you do this, you'll need to comment out any
# sections that require sensitive data.
#merged <- readRDS("./data/public/merged_deidentified.rds")

# Read in data from experimenter survey (Google Form - private due to sensitive info)
exp_surv <- readRDS("./data/raw_site_data/experimenter survey/exp_surv.rds")
```

```{r analysis-participants, include = FALSE}

# The 'merged' df includes all data provided by sites
# I'm going to retain it in case we need to refer to it later, and then
# apply the study-wide exclusion criteria noted below.

merged_original <- merged

# Aggregate participants characteristics
# Converting to numeric
merged$age <- as.numeric(as.character(merged$age))
merged$gender <- as.numeric(as.character(merged$gender))
merged$race <- as.numeric(as.character(merged$race))

# Applying exclusion criteria 1
merged <- filter(merged, pass_ER1 == T)

n_woman <- length(which(merged$gender== 1)) #number of women
n_man <- length(which(merged$gender== 2)) #number of men
n_other <- length(which(merged$gender== 3)) #other responses

n_woman_pct <- length(which(merged$gender== 1))/nrow(merged)*100 #pct women
n_man_pct <- length(which(merged$gender== 2))/nrow(merged)*100 #pct men
n_other_pct <- length(which(merged$gender== 3))/nrow(merged)*100 #pct other/blank

n_white <- length(which(merged$race == 1)) #num White
n_white_pct <- length(which(merged$race == 1))/nrow(merged)*100 #percent White, using the length of the source variable (assigned to all sessions) for total N
n_black <- length(which(merged$race == 2)) #num Black or African American 
n_black_pct <- length(which(merged$race == 2))/nrow(merged)*100 #percent Black
n_aian <- length(which(merged$race == 3)) #num American Indian or Alaska Native
n_aian_pct <- length(which(merged$race == 3))/nrow(merged)*100 #percent American Indian/Alaska Native
n_asian <- length(which(merged$race == 4)) #num Asian
n_asian_pct <- length(which(merged$race == 4))/nrow(merged)*100 #percent Asian
n_haw <- length(which(merged$race == 5)) #num Native Hawaiian or Pacific Islander
n_haw_pct <- length(which(merged$race == 5))/nrow(merged)*100 #percent Native Hawaiian or Pacific Islander
n_other <- length(which(merged$race == 6)) #num Other
n_other_pct <- length(which(merged$race == 6))/nrow(merged)*100 #percent other

```

# Results

```{r analysis-replication-meta, include = FALSE}

# In 002_ml4analysis.R I generate results per site, using the same analysis
# but each of the three exclusion rules. Those are output to:
# ./data/public/combinedresults1.csv
# ./data/public/combinedresults2.csv
# ./data/public/combinedresults3.csv

# Here I'll read in those files, but if you're error checking you'll also
# want to review the code in 002_ml4analysis.R that generates them

combinedresults0 <- read.csv("./data/public/combinedresults0.csv")
combinedresults1 <- read.csv("./data/public/combinedresults1.csv")
combinedresults2 <- read.csv("./data/public/combinedresults2.csv")
combinedresults3 <- read.csv("./data/public/combinedresults3.csv")

# analyses repeated for each set of exclusion criteria
# three-level random-effects meta-analysis in MetaSEM
# summary( meta3(y=yi, v=vi, cluster=location, data=combinedresults0)) #line not necessary, results for a subset we never use (e.g., zero exclusions)
random_effects_1 <- summary(random1 <- meta3(y=yi, v=vi, cluster=location, data=combinedresults1))
random_effects_2 <- summary(random2 <- meta3(y=yi, v=vi, cluster=location, data=combinedresults2))
random_effects_3 <- summary(random3 <- meta3(y=yi, v=vi, cluster=location, data=combinedresults3))
#Notes: Q statistic is for sig test for heterogeneity among all effect sizes. I2 for level 2 indicates the percent of total variance explained by effects within sites, and I2 for level 3 indicates the percent of total variance accounted for by differences between sites. Intercept is the avg population effect. 

# a covariate of study version (in-house or expert-designed) is added to create a three-level mixed-effects meta-analysis
# note the openMX status, sometimes indicates a potential problem
# summary( mixed0 <- meta3(y=yi, v=vi, cluster=location, x=expert, data=combinedresults0)) #line not necessary, results for a subset we never use (e.g., zero exclusions)
mixed_effects_1 <- summary(mixed1 <- meta3(y=yi, v=vi, cluster=location, x=expert, data=combinedresults1))
mixed_effects_2 <- summary(mixed2 <- meta3(y=yi, v=vi, cluster=location, x=expert, data=combinedresults2))
mixed_effects_3 <- summary(mixed3 <- meta3(y=yi, v=vi, cluster=location, x=expert, data=combinedresults3))
# Notes: The R? for the version predictor will be reported for both level 2 and level 3, although in this case version is a level 2 predictor so the level 3 R? will always be zero. 

# constraining the variance to test if it significantly worsens the model
# summary( fixed0 <- meta3(y=yi, v=vi, cluster=location, x=expert, data=combinedresults0, RE2.constraints=0, RE3.constraints=0)) #line not necessary, results for a subset we never use (e.g., zero exclusions)
constrained_1 <- summary(fixed1 <- meta3(y=yi, v=vi, cluster=location, x=expert, data=combinedresults1, RE2.constraints=0, RE3.constraints=0))
constrained_2 <- summary(fixed2 <- meta3(y=yi, v=vi, cluster=location, x=expert, data=combinedresults2, RE2.constraints=0, RE3.constraints=0))
constrained_3 <- summary(fixed3 <- meta3(y=yi, v=vi, cluster=location, x=expert, data=combinedresults3, RE2.constraints=0, RE3.constraints=0))

# compare if there is a significant difference in model fit, chi square difference test
# anova(mixed0, fixed0)
fit_comparison_1 <- anova(mixed1, fixed1)
fit_comparison_2 <- anova(mixed2, fixed2)
fit_comparison_3 <- anova(mixed3, fixed3)
```

## Research Question 1: Meta-analytic results across all labs (random effects meta-analysis). 
The most basic question is whether we observed the predicted effect of mortality salience on preference for pro- vs anti- American essay authors. To assess this we conducted a three-level random-effects meta-analysis.[^4] This analysis produces the grand mean effect size across all sites and versions. Regardless of which exclusion criteria were used, we did not observe the predicted effect and the confidence interval was quite narrow: 
Exclusion Set 1: `r report_meta3(random1, "Intercept")`. 
Exclusion Set 2: `r report_meta3(random2, "Intercept")`. 
Exclusion Set 3: `r report_meta3(random3, "Intercept")`. 
Forest plots showing the effects for individual sites and the aggregate are available in Figure 1 for Exclusion Set 1 (see https://osf.io/8ccnw/ for the other two Exclusion Sets).

[^4]: Sample code to run this analysis is: meta3(y=es, v=var, cluster=Location, data=dataset). In this sample code, ‚Äúy=es‚Äù directs the program to the column of effect sizes, ‚Äúv=var‚Äù indicates the variable to be used as the sampling variance for each effect size, and the ‚Äúcluster=Location‚Äù command groups the effect sizes by a location variable in the dataset (in this case, a unique identifier assigned to each replication site).

There may have been a mortality salience effect at some sites and not others, so we next examined how much variation was observed among effect sizes (e.g., heterogeneity). 

For Exclusion Sets 1 and 3, this sort of variation did not exceed variation expected by chance (e.g., sampling variance): Exclusion Set 1: `r report_q(random1)`; Exclusion Set 2: `r report_q(random2)`; Exclusion Set 3: `r report_q(random3)`. 

In sum, we observed little evidence for an overall effect of mortality salience in these replications. And, overall results suggest that there was minimal or no heterogeneity in effect sizes across sites. This lack of variation suggests that it is unlikely we will observe an effect of Author Advised versus In House protocols or other moderators such as differences in samples or TMT knowledge. Even so, the plausible moderation by Author Advised/In House protocol is examined in the following section. 

## Research Question 2: Moderation by Author Advised/In House protocol
A covariate of protocol type was added to the random effects model to create a three-level mixed-effects meta-analysis. This was pre-registered as our primary analysis.[^5] 

[^5]: The addition of the argument "x = version" to the prior metaSEM R code can be seen here: meta3(y=es, v=var, cluster=Location, x=version, data=dataset)

This analysis again produces an overall grand mean effect size, and those were again near zero and relatively precisely estimated across all three Exclusion Sets: Exclusion Set 1: `r report_meta3(mixed1, "Intercept")`. Exclusion Set 2: `r report_meta3(mixed2, "Intercept")`. Exclusion Set 3: `r report_meta3(mixed3, "Intercept")`.

Variation among effect sizes also followed the previously observed pattern. Weak heterogeneity for Exclusion Set 2, *Q*(`r mixed_effects_2$Q.stat$Q.df`) = `r mixed_effects_2$Q.stat$Q`, *p* = `r mixed_effects_2$Q.stat$pval`, Tau^2^~within\ labs~ = `r mixed_effects_2$coefficients["Tau2_2", "Estimate"]`, Tau^2^~between\ labs~ = `r mixed_effects_2$coefficients["Tau2_3", "Estimate"]`; while variation did not meet the statistical significance threshold for Exclusion Set 1 *Q*(`r mixed_effects_1$Q.stat$Q.df`) = `r mixed_effects_1$Q.stat$Q`, *p* = `r mixed_effects_1$Q.stat$pval`; or Exclusion Set 3: *Q*(`r mixed_effects_3$Q.stat$Q.df`) = `r mixed_effects_3$Q.stat$Q`, *p* = `r mixed_effects_3$Q.stat$pval`.

Critically, protocol version did not significantly predict replication effect size regardless of which exclusion criteria were used. Exclusion Set 1: *b* = `r mixed_effects_1$coefficients["Slope_1", "Estimate"]`, *Z* = `r mixed_effects_1$coefficients["Slope_1", "z value"]`, *p* = `r mixed_effects_1$coefficients["Slope_1", "Pr(>|z|)"]`; Exclusion Set 2: *b* = `r mixed_effects_2$coefficients["Slope_1", "Estimate"]`, *Z* = `r mixed_effects_2$coefficients["Slope_1", "z value"]`, *p* = `r mixed_effects_2$coefficients["Slope_1", "Pr(>|z|)"]`; Exclusion Set 3: *b* = `r mixed_effects_3$coefficients["Slope_1", "Estimate"]`, *Z* = `r mixed_effects_3$coefficients["Slope_1", "z value"]`, *p* = `r mixed_effects_3$coefficients["Slope_1", "Pr(>|z|)"]`. The Author Advised version did not produce larger effect sizes when compared with the In House versions.

## Research Question 3: Effect of Standardization
Finally, we tested whether In House protocols displayed greater variability in effect size than Author Advised protocols. To test this hypothesis, we ran the mixed-effects models but constrained the variances at both Level 2 and Level 3 to 0, effectively creating fixed-effects models. These models were then compared with a chi-squared differences test to assess whether the fit significantly changed. In this case, none of the three models significantly decreased in fit: Exclusion Set 1: *$\chi$¬≤* (`r fit_comparison_1$diffdf[2]`) = `r fit_comparison_1$diffLL[2]`, *p* = `r fit_comparison_1$p[2]`; Exclusion Set 2: *$\chi$¬≤* (`r fit_comparison_2$diffdf[2]`) = `r fit_comparison_2$diffLL[2]`, *p* = `r fit_comparison_2$p[2]`; Exclusion Set 3: *$\chi$¬≤* (`r fit_comparison_3$diffdf[2]`) = `r fit_comparison_3$diffLL[2]`, *p* = `r fit_comparison_3$p[2]`. Overall, there was no evidence that In House protocols elicited greater variability than Author Advised protocols despite the fact that they were unambiguously more variable in their procedural implementation.

```{r analysis-exploratory-knowl, include = FALSE}
# Focused analysis of sites with "expert" or "a lot of knowledge about TMT" leads
# Still using exclusion set 1 (the 'merged' datafile), implemented in participants section

# Selecting only the below sites:
#University of Wisconsin, Madison, WI (in-house)
#The College of New Jersey
#University of Kansas (Expert)
#University of Kansas (in-house)
#Pace University (expert)
#Virginia Commonwealth University, Richmond, VA
data_knowledgeable <- subset(merged, merged$source=="uwmadison_inhouse" | merged$source=="cnj" | merged$source=="kansas_expert" | merged$source=="kansas_inhouse" | merged$source=="pace_expert" | merged$source=="vcu")

#uwmadison_inhouse used a 7 point scale, but with similar anchors, to the 9-point scale used by the other sites. To make this direct comparison I'm going to scale the DV to a 9-point scale. 

data_knowledgeable[data_knowledgeable$source == "uwmadison_inhouse", 'pro_minus_anti'] <- data_knowledgeable[data_knowledgeable$source == "uwmadison_inhouse", 'pro_minus_anti'] * (9/7)

# Applying the same levels fix as earlier, only because it caused problems in 
# cohen.d() below. May not be necessary anymore.
data_knowledgeable$ms_condition <- factor(data_knowledgeable$ms_condition, levels = c("ms", "tv"))
# Analyses using that subset
knowl_ttest <- t.test(data_knowledgeable$pro_minus_anti~data_knowledgeable$ms_condition)
knowl_desc <- describeBy(data_knowledgeable$pro_minus_anti, group = data_knowledgeable$ms_condition)
knowl_effsize <- effsize::cohen.d(data_knowledgeable$pro_minus_anti~data_knowledgeable$ms_condition,pooled=TRUE,paired=FALSE,na.rm=TRUE, hedges.correction=TRUE,conf.level=0.95)
```

## Follow-Up Exploratory Analyses
**Results for TMT-knowledgeable sites.** One principal investigator reported being an expert in TMT, while five others indicated having ‚Äúa lot‚Äù of knowledge about TMT. One might expect that these locations would have greater success at replicating the mortality salience effect. Aggregating across these sites, and using only the first exclusion rule, these sites did not elicit a larger difference between the mortality salience group (*M* = `r as.numeric(knowl_desc$ms["mean"])`, *SD* = `r as.numeric(knowl_desc$ms["sd"])`) and the control group (*M* = `r as.numeric(knowl_desc$tv["mean"])`, *SD* = `r as.numeric(knowl_desc$tv["sd"])`), *t*(`r knowl_ttest$parameter`) = `r knowl_ttest$statistic`, *p* = `r knowl_ttest$p.value`, *Hedges‚Äô g* = `r knowl_effsize$estimate`, 95% CI = [`r knowl_effsize$conf.int["lower"]`, `r knowl_effsize$conf.int["upper"]`].[^6] 

[^6]: One site, UW Madison In House, used a 7-point scale. This has been rescaled to a 9-point scale for this analysis to approximately compare it with the others.

```{r analysis-exploratory-preferpro, include = FALSE}
# Investigating only participants who reported preference for the pro-US author.

# generate dfs for Author Advised and In House sites.
merged_aa <- filter(merged, expert == 1)
merged_ih <- filter(merged, expert == 0)

### Percent rating pro-author more highly than anti-author, basic exclusions
# In House
n_profav_ih <- sum(merged_ih$pro_minus_anti > 0)
n_antifav_ih <- sum(merged_ih$pro_minus_anti < 0)
n_nofav_ih <- sum(merged_ih$pro_minus_anti == 0)

pct_profav_ih <- (n_profav_ih/(n_profav_ih+n_antifav_ih+n_nofav_ih))*100
pct_antifav_ih <- (n_antifav_ih/(n_profav_ih+n_antifav_ih+n_nofav_ih))*100
pct_nofav_ih <- (n_nofav_ih/(n_profav_ih+n_antifav_ih+n_nofav_ih))*100

# Author Advised
n_profav_aa <- sum(merged_aa$pro_minus_anti > 0)
n_antifav_aa <- sum(merged_aa$pro_minus_anti < 0)
n_nofav_aa <- sum(merged_aa$pro_minus_anti == 0)

pct_profav_aa <- (n_profav_aa/(n_profav_aa+n_antifav_aa+n_nofav_aa))*100
pct_antifav_aa <- (n_antifav_aa/(n_profav_aa+n_antifav_aa+n_nofav_aa))*100
pct_nofav_aa <- (n_nofav_aa/(n_profav_aa+n_antifav_aa+n_nofav_aa))*100

# Subset to Author Advised participants who preferred the pro-US author 
# and examine if that finds the effect. Repeat for 3 exclusion sets.
# Datasets:
# merged_aa is basic exclusions
# merged_excl_2_aa is exclusion set 2
merged_excl_2_aa <- filter(merged_excl_2, expert == 1)
# merged_excl_3_aa is exclusion set 3
merged_excl_3_aa <- filter(merged_excl_3, expert == 1)

# Analyses for each
merged_aa_ttest <- t.test(merged_aa$pro_minus_anti~merged_aa$ms_condition)
merged_aa_desc <- describeBy(merged_aa$pro_minus_anti, group = merged_aa$ms_condition)
merged_aa_effsize <- effsize::cohen.d(merged_aa$pro_minus_anti~merged_aa$ms_condition,pooled=TRUE,paired=FALSE,na.rm=TRUE, hedges.correction=TRUE,conf.level=0.95)

# Analyses using that subset
merged_excl_2_aa_ttest <- t.test(merged_excl_2_aa$pro_minus_anti~merged_excl_2_aa$ms_condition)
merged_excl_2_aa_desc <- describeBy(merged_excl_2_aa$pro_minus_anti, group = merged_excl_2_aa$ms_condition)
merged_excl_2_aa_effsize <- effsize::cohen.d(merged_excl_2_aa$pro_minus_anti~merged_excl_2_aa$ms_condition,pooled=TRUE,paired=FALSE,na.rm=TRUE, hedges.correction=TRUE,conf.level=0.95)

# Analyses using that subset
merged_excl_3_aa_ttest <- t.test(merged_excl_3_aa$pro_minus_anti~merged_excl_3_aa$ms_condition)
merged_excl_3_aa_desc <- describeBy(merged_excl_3_aa$pro_minus_anti, group = merged_excl_3_aa$ms_condition)
merged_excl_3_aa_effsize <- effsize::cohen.d(merged_excl_3_aa$pro_minus_anti~merged_excl_3_aa$ms_condition,pooled=TRUE,paired=FALSE,na.rm=TRUE, hedges.correction=TRUE,conf.level=0.95)

```

**Results for participants who preferred the pro-US author** The present hypothesis that mortality salience would cause a participant to become more favorable to the pro-US author as compared to the anti-US author relies on the participant perceiving the pro-US stance as more similar to their own worldview (and/or the anti-US stance as threatening to their worldview). Original authors anticipated that the essays from the original study may not serve this function in the replication, run in 2016. For this reason, the anti-US essay from the original study was made more extreme in the Author Advised version of the replication. There was a particular concern that in the months leading up to and following the 2016 US Presidential Election of Donald Trump, the generally more liberal-leaning student bodies on college campuses may feel less patriotic and not identify with the pro-US worldview. Indeed, analysis suggests the original authors anticipated and more successfully addressed this issue. Among In House replications, `r round(pct_profav_ih, digits = 0)`% of participants prefered the pro-US essay author, `r round(pct_antifav_ih, digits = 0)`% preferred the anti-US essay author, and `r round(pct_nofav_ih, digits = 0)`% had no preference. Among Author Advised replications, `r round(pct_profav_aa, digits = 0)`% of participants prefered the pro-US essay author, `r round(pct_antifav_aa, digits = 0)`% preferred the anti-US essay author, and `r round(pct_nofav_aa, digits = 0)`% had no preference.

However, the predicted mortality salience effect was not larger or detectable via statistical significance when subsetting to only participants at Author Advised sites who preferred the pro-US author. In all exclusion sets, the mortality salience and control groups showed similar levels of preference for the pro-US author over the anti-US author: Exclusion Set 1: mortality salience group (*M* = `r as.numeric(merged_aa_desc$ms["mean"])`, *SD* = `r as.numeric(merged_aa_desc$ms["sd"])`), control group (*M* = `r as.numeric(merged_aa_desc$tv["mean"])`, *SD* = `r as.numeric(merged_aa_desc$tv["sd"])`), *t*(`r merged_aa_ttest$parameter`) = `r merged_aa_ttest$statistic`, *p* = `r merged_aa_ttest$p.value`, *Hedges‚Äô g* = `r merged_aa_effsize$estimate`, 95% CI = [`r merged_aa_effsize$conf.int["lower"]`, `r merged_aa_effsize$conf.int["upper"]`]; Exclusion Set 2: mortality salience group (*M* = `r as.numeric(merged_excl_2_aa_desc$ms["mean"])`, *SD* = `r as.numeric(merged_excl_2_aa_desc$ms["sd"])`), control group (*M* = `r as.numeric(merged_excl_2_aa_desc$tv["mean"])`, *SD* = `r as.numeric(merged_excl_2_aa_desc$tv["sd"])`), *t*(`r merged_excl_2_aa_ttest$parameter`) = `r merged_excl_2_aa_ttest$statistic`, *p* = `r merged_excl_2_aa_ttest$p.value`, *Hedges‚Äô g* = `r merged_excl_2_aa_effsize$estimate`, 95% CI = [`r merged_excl_2_aa_effsize$conf.int["lower"]`, `r merged_excl_2_aa_effsize$conf.int["upper"]`]; Exclusion Set 3: mortality salience group (*M* = `r as.numeric(merged_excl_3_aa_desc$ms["mean"])`, *SD* = `r as.numeric(merged_excl_3_aa_desc$ms["sd"])`), control group (*M* = `r as.numeric(merged_excl_3_aa_desc$tv["mean"])`, *SD* = `r as.numeric(merged_excl_3_aa_desc$tv["sd"])`), *t*(`r merged_excl_3_aa_ttest$parameter`) = `r merged_excl_3_aa_ttest$statistic`, *p* = `r merged_excl_3_aa_ttest$p.value`, *Hedges‚Äô g* = `r merged_excl_3_aa_effsize$estimate`, 95% CI = [`r merged_excl_3_aa_effsize$conf.int["lower"]`, `r merged_excl_3_aa_effsize$conf.int["upper"]`]. The confidence intervals were wider because of the smaller total sample size, but this evidence is not consistent with the hypothesis that preference for the pro-US author would elicit an effect of mortality salience in this context. 

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
